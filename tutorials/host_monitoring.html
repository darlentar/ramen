<html>
  <head>
    <base href="https://rixed.github.io/ramen/">
  </head>
<body>

  <script type="application/json" class="js-hypothesis-config">{ "showHighlights": "whenSidebarOpen" }</script>
  <script src="https://hypothes.is/embed.js" async></script>

  <div id="menu">
    <a href="index.html">Overview</a>
    <a href="download.html">Downloading</a>
    <a href="build.html">Building</a>
    <a href="tutorials.html">Tutorials</a>
    <a href="design.html">Design</a>
    <a href="man.html">Command line reference</a>
    <a href="language_reference.html">Language reference</a>
    <a href="blog.html">Blog posts</a>
  </div>
<h1>Host Monitoring in 15 minutes</h1>

<p><b>WARNING:</b> This is for an old version of Ramen and goes too much into details. To be replaced by a shorter tutorial which objective should be to quickly get the idea of what ramen is.</p>

<h2>Starting up</h2>

<h3>Using the Docker image</h3>

<p>This tutorial will assume that you've created the directory <code>$HOME/ramen_root</code> to store the programs that are going to be written and share them with the docker image:</p>

<pre>
$ mkdir -p $HOME/ramen_root
</pre>

<p>The docker image is about 150MiB compressed. Run it with:</p>

<pre>
$ docker run --name=ramen-test \
    -v $HOME/ramen_root:/ramen/ramen_root \
    -p 25826:25826/udp -p 2055:2055/udp \
    rixed/ramen:demo
</pre>

<p>This will mount your (currently empty) <code>ramen_root</code> directory into the docker image, redirect <em>collectd</em> and <em>netflow</em> UDP ports into the docker image, and start Ramen process supervisor in a docker container, logging to stderr.  Leave it there and continue this tutorial from another terminal.</p>

<p>Redirecting the ports is optional: The docker image comes with <a href="https://collectd.org/">collectd</a> and <a href="http://fprobe.sourceforge.net/">fprobe</a> daemons that will monitor the container itself, but you could also points external collectd and netflow sources to this port.</p>

<p>From now on we will run the `ramen` program through docker so often that a shortcut is in order:</p>

<pre>
$ alias ramen "docker exec ramen-test ramen"
</pre>

<h3>Installing more probes</h3>

<p>Skip that section if you are happy with using only internally generated metrics for the duration of this tutorial.</p>

<h4>Installing collectd</h4>

<p><code>aptitude install collectd</code>.</p>

<p>Edit <code>/etc/collectd/collectd.conf</code> (or the equivalent, such as <code>/usr/local/etc/collectd.conf</code> if brewed on mac OS), uncomment `LoadPlugin network` and make sure the stats will be sent to your server running ramen. For now let's say you run collectd and ramen on the same host, that means you must have this configuration for the network plugin:</p>

</pre>
<Plugin network>
  Server "127.0.0.1" "25826"
</Plugin>
</pre>

<p>WARNING: Replace "127.0.0.1" with the actual address of your host if collectd runs elsewhere.</p>

<p>Then you can restart collectd (<code>systemctl restart collectd.service</code> or <code>/usr/local/sbin/collectd -f -C /usr/local/etc/collectd.conf</code> or whatever works for you).</p>

<h4>Sending more netflow to Ramen</h4>

<p>The docker image comes with an internal netflow source that will send flow information to Ramen about the traffic taking place inside the container.  This is of course of little use for monitoring but is enough to get some data.</p>

<p>If you already have access to a switch that can emit netflow statistics then you could direct them to the container port 2055 for more interesting data.</p>

<h2>Getting some data in</h2>

<p>For Ramen to do anything useful you need to write some programs for it.  As a first program we are going to collect netflow statistics and turn it into a small network traffic dashboard.</p>

<p>Let's create a Ramen source file named <code>demo.ramen</code> in that <code>$HOME/ramen_root</code> directory with this content:</p>

<pre>
-- Ingest some data from the outside world:
DEFINE netflow AS listen for netflow;
DEFINE collectd AS listen for collectd;
</pre>

<p>Note that comments are introduced with <code>--</code> as in SQL, and also like SQL ramen language is mostly case insensitive (but for field and operation names though).</p>

<p>This program thus defines two operations, named `netflow` and `collectd`, with whatever content will be received using those protocols.</p>

<p>You can then compile this file with:</p>

<pre>
$ ramen compile ramen_root/demo.ramen --as demo
</pre>

<p>...and if all goes well you should get see something along those lines:</p>

<pre>
10h35m34: Parsing program demo
10h35m34: Typing program demo
10h35m34: Compiling for program demo
10h35m34: Compiling "/root/demo_bdbdfbabfe4ad6b3f387f95b4945423f.ml"
10h35m34: Compiling "/root/demo_9cecc492bd21247f054b8f85f8335637.ml"
10h35m34: Linking "root/demo_casing.ml"
</pre>

<p>...informing you that your program has been successfully parsed, typed, compiled and linked. You should now have a <code>demo.x</code> executable file in your <code>ramen_root</code> directory. You could try to run it but that would not give anything useful at that stage:</p>

<pre>
$ ~/ramen_root/demo.x
(... binary vomit redacted ...)
</pre>

<p>What was that? Is it broken already?  The binary string that's output by that executable has very little to do with gathering monitoring data. Instead, it was just the worker dumping its configuration at you as if you were Ramen's process supervisor.</p>

<p>NOTE: If you are interested, run it again with <code>--Im-a-human</code> for a human friendlier version of the same configuration.</p>

<p>Workers are not supposed to be run like that. Instead, you must ask ramen to run it:</p>

<pre>
$ ramen run root/demo.x
</pre>

<p>NOTE: here we give the path to the binary inside the container so that ramen will find it.</p>

<p>A quick look at `ramen ps` confirms that it's indeed running:</p>

<pre>
$ ramen ps | less -x 16 -S
</pre>

<p>Of course good old plain `ps` would also confirm it once you know where to look but `ramen ps` also gives you statistics about the number of received and emitted tuples and so on.</p>

<h2>Getting data out</h2>

<p>Let's see those netflow tuples:</p>

<pre>
$ ramen tail --with-header demo/netflow
</pre>

<p>...and then wait a bit and tuples should arrive. Stop by sending the INT signal (control-C).</p>

<p>Let's now extract a timeseries for the <code>bytes</code> field. Assuming you are using GNU date and a Bourne-like shell, you could type:</p>

<pre>
$ ramen timeseries \
    --since $(date -d '10 minutes ago' '+%s') \
    --until $(date -d '5 minutes ago' '+%s') \
    --nb-points 30 --consolidation sum \
    demo/netflow bytes
</pre>

<p>NOTE: The reason why we specify an `until` date that far in the past is because the netflow protocol would send us information about events that are already past. If you omit the <code>--until</code> option ramen will assume you want data up to now, and will wait for the last received event to have a starting time greater than now. Depending on your switch configuration you would therefore have to wait from tens of seconds to several minutes.</p>

<p>The <code>consolidation</code> option specify how to fit events into the time buckets, and possible values are <code>min</code>, <code>max</code>, <code>avg</code> (the default) and <code>sum</code>. Here we are accumulating traffic volumes from different sources so the only meaningful way to combine those volumes is to sum them (averaging would yield the average number of bytes per netflow for each time bucket, which is of little significance).</p>

<p>That timeseries of course could be piped into any dashboarding program, such as the venerable <code>gnuplot</code>:</p>

<pre>
$ while sleep 10; do \
    ramen timeseries \
      --since $(date -d '10 minutes ago' '+%s') \
      --until $(date -d '5 minutes ago' '+%s') \
      --nb-points 30 --separator ' ' --null 0 --consolidation sum \
      demo/netflow bytes | \
    gnuplot -p -e "set timefmt '%s'; set xdata time; set format x '%H:%M'; \
      set terminal dumb $COLUMNS,$LINES; \
      plot '&lt; cat -' using 1:2 with lines title 'Bytes'";
  done

  18000 +-+--+-+-+*+--+-+-+-+--+-+-+-+--+-+-+--+-+-+-+--+-+-+-+--+-+-+-+--+-+
        +      +  *   +     +      +      +      +      +     +      +      +
        |        * *                                          Bytes ******* |
  16000 +-+      * *                                                      +-+
        |        *  *                                                       |
        |*       *  *                                                       |
  14000 +-*  *  *   *                                                     +-+
        | * * * *   *                                                       |
        |  ** * *    *                           *                          |
  12000 +-+*   *     *                *          **                       +-+
        |      *     *               **          * *                        |
  10000 +-+          *               * *        *   ***                   +-+
        |            *         *    *  *        *     *          *    *     |
        |             *  *     **   *  *        *      *        * *  * *  * |
   8000 +-+           ****    *  * *    * ****  *      *      **   * * * *+-+
        |             *   *   *  * *    **   *  *       *   **      *   *   |
        |                 *  *    *           * *       ****                |
   6000 +-+                **                 **                          +-+
        |                  *                   *                            |
        +      +      +     +      +      +    * +      +     +      +      +
   4000 +-+--+-+-+-+--+-+-+-+--+-+-+-+--+-+-+--+-+-+-+--+-+-+-+--+-+-+-+--+-+
      12:53  12:53  12:54 12:54  12:55  12:55  12:56  12:56 12:57  12:57  12:58
</pre>

<p>Ok, now that we are confident we know how to get some data in and out, let's have a look at what we can do with the data in between.</p>

<h2>Ramen Programs and Operations</h2>

<p>Programs are sets of operations. An operation can be of several types: listening to some network port for some known protocol (such as collectd or netflow above) is one of them. In general though, operations will consist of SQL-like stanzas manipulating tuples (<code>SELECT foo + bar, baz FROM another_operation WHERE foo = 42...</code>).  <em>Tuples</em> are like a row in SQL: a collection of named fields and values. Values can have various types (integers, strings, booleans...) as in SQL. For instance, here is a tuple:</p>

<table>
<tr>
  <th>time</th>
  <th>host</th>
  <th>interface</th>
  <th>sent</th>
  <th>received</th>
</tr><tr>
  <td>1507295705.54</td>
  <td>www45</td>
  <td>em0</td>
  <td>749998080</td>
  <td>1821294592</td>
</tr>
</table>

<p>It is frequent to refer to tuples as <em>events</em> and we might use one or the other term.</p>

<p>In a stream processor, operations are chained together and run forever (in theory). In Ramen, operations have <em>parents</em> and <em>children</em>. An operation sends the tuple it produces to each of its children.</p>

<p>Programs are the granularity at which operations can be created, started and stopped.  Within a program you can build loops of operations. Outside of programs, though, loops are not allowed: when you add a program, all the parent operations must either be in the program you are adding or in a program that's already defined.</p>

<p>Operations and programs have names. Program names must be globally unique while operation names need only be unique within the program they belong to. The <em>fully qualified</em> name of an operation is the name of the program it belongs to, followed by a slash ("/"), followed by the name of the operation. Consequently, the slash character is not allowed in an operation name.</p>

<p>For instance, "base/per_hosts/hourly_traffic" is the fully qualified name of the operation "hourly_traffic" in the program named "base/per_hosts". Notice that the slash ("/") in the program name is just a convention with no particular meaning.</p>

<p>For now we have a single program named "demo" containing only two operations.</p>

<h2>Computing Memory Consumption</h2>

<p>Monitoring usually involves three phases:</p>

<ol>
<li>Collect all possible data (that's what we have just done above);</li>
<li>Turn that data into meaningful information;</li>
<li>Finally alert on that information.</li>
</ol>

<p>We are now going to see how we could turn our netflows and collectd messages into something useful.</p>

<p>Collectd events are very fine grained and one may want to build a more synthetic view of the state of some subsystem. Let's start with memory: Instead of having individual events with various bits of information about many subsystems, let's try to build a stream representing, at a given time, how memory is allocated for various usage.</p>

<p>So to begin with, let's filter the events generated by collectd memory probes.  Let's write a new program and call it <code>hosts.ramen</code>, for we will monitor hosts health in it.</p>

<pre>
DEFINE memory AS
  SELECT * FROM demo/collectd WHERE plugin = "memory"
  EVENT STARTING AT time;
</pre>

<p>Without going too deep into Ramen syntax, the intended meaning of this simple operation should be clear: we want to filter the tuples according to their <code>plugin</code> field and keep only those originating from the "memory" plugin.  The <code>EVENT ...</code> part is required to extract a timeseries from the tuples, which is the first step toward plotting the tuples (that's where ramen learns the event time from).</p>

<p>NOTE: The <code>STARTING AT ...</code> bit means that, when we plot the output then the timestamp for these tuples are to be taken in the field called <code>time</code>.  In many stream processors time is a hardcoded field of a specific format. In some others, event time is even assumed to be current time (ie. the time the event has been generated is assumed to be the time it as been received by the stream processor). With Ramen time is not mandatory and can have any format that floats your boat. You can even have both a starting time and an ending time for each tuple. The price to pay for this flexibility is that, should you intend to plot the tuples or use any function that requires the time, you then have to instruct Ramen how to get the time from the events.</p>

<p>If you try to compile the above program though, you should get an error message which, if you are used to SQL, might surprise you:</p>

<pre>
$ ramen compile --root=root root/hosts.ramen
15h39m57: Parsing program hosts
15h39m57: Typing program hosts
15h39m57: Exception: In function memory: equality must not be nullable
</pre>

<p>What is this equality and why must not it be nullable? What does that even mean to be nullable?</p>

<p>A value is <em>nullable</em> if it can be <em>null</em>. Null is the SQL traditional equivalent of the dreadful NULL pointer of C. The NULL value (which really should be called "UNKNOWN" rather than "NULL") is a value that contaminate all other values combined with it. For instance, <code>NULL + 1</code> is NULL, and so is <code>NULL = 1</code>. So, consider the expression <code>a = b</code>: if either of <code>a</code> or <code>b</code> can be NULL, so can <code>a = b</code>. So the type of <code>a = b</code> can be either a boolean or a <em>nullable</em> boolean, depending on <code>a</code> and <code>b</code>.</p>

<p>In the above operation there is only one equality operator: <code>plugin = "memory"</code>. Of course the constant string <code>"memory"</code> cannot be NULL (the only nullable constant is <code>NULL</code> itself, which is not only nullable but, of course, actually always null). So <code>plugin</code> might be nullable? Indeed, despite the plugin of a collectd message is rarely unset, the fact is the collectd protocol does <em>not</em> mandate this value to be defined. As a consequences, ramen reserves the possibility to set it to NULL in case it ever receives a message from collectd with an unset plugin value. It seems contamination by NULL traverses program boundaries!</p>

<p>Now, why isn't this equality allowed to be nullable?  Because it is the `where` clause. What should Ramen do, if the filter condition ever returns NULL? There is no good decision to be made, and that is why Ramen enforces every `where` clauses to be non-nullable booleans.  Correct typing is an important design goal of Ramen so that it can be reliably used to deliver alerts (its primary intended purpose).  In particular, it is impossible to draw a NULL value whenever it makes no sense.  Better working around this restriction now than to encounter a NULL there later in production.</p>

<p>So, what shall we do when <code>plugin</code> is null? It seems reasonable to assume that an information that's lacking a plugin information is not originating form the memory plugin, and thus can be filtered out. To this end, we must use the <code>COALESCE</code> operator, which is (currently) the only way to get rid of nullability. As in SQL, "coalesce" takes a list of expressions and returns the first one that is not null.  In Ramen there are additional constraints though: this list of expressions cannot be empty, the last expression is not allowed to be nullable, while every others must be ; so that it is guaranteed that the result of a coalesce is never null.</p>

<p>So, edit the memory operation to look like this:</p>

<pre>
DEFINE memory AS
  SELECT * FROM demo/collectd WHERE COALESCE(plugin = "memory", false)
  EVENT STARTING AT time;
</pre>

<p>Save it and you should now be able to compile and run it.</p>

<p>You might notice (<code>ramen tail hosts/memory</code>) that this plugin only sets one value and also that the <code>type_instance</code> field contains the type of memory this value refers to.  Apart from that, most fields are useless. We could therefore make this more readable by changing its operation into the following, enumerating the fields we want to keep (and implicitly discarding the others):</p>

<pre>
DEFINE memory AS
  SELECT time, host, type_instance, value
  FROM demo/collectd
  WHERE COALESCE(plugin = "memory", false)
  EVENT STARTING AT time;
</pre>

<p>The output is now easier to read; it should look something like this:</p>

<pre>
$ ramen tail hosts/memory --with-header
#time,host,type_instance,value
1522945763.3,"poum","used",4902309888
1522945763.3,"poum","cached",17255350272
1522945763.3,"poum","buffered",2819915776
1522945763.3,"poum","free",763043840
1522945763.3,"poum","slab_unrecl",97742848
1522945763.3,"poum","slab_recl",7081136128
1522945773.3,"poum","used",4902801408
1522945773.3,"poum","cached",17255350272
1522945773.3,"poum","buffered",2819915776
1522945773.3,"poum","slab_recl",7081103360
1522945773.3,"poum","slab_unrecl",97460224
1522945773.3,"poum","free",762867712
...
</pre>

<p>On your own system, other "type instances" might appear; please adapt accordingly as you read along.</p>

<p>There is still a major annoyance though: we'd prefer to have the values for each possible "type instances" (here: the strings "free", "used", "cached" and so on) as different columns of a single row, instead of spread amongst several rows, so that we know at each point in time what the memory usage is like.  Since we seem to receive one report form collectd every 10 seconds or so, a simple way to achieve this would be to accumulate all such reports for 30 seconds and then output a single tuple every 30 seconds with one column per known "type instance".</p>

<p>For this, we need to "aggregate" several tuples together, using a <code>GROUP BY</code> clause. Try this:</p>

<pre>
DEFINE memory AS
  SELECT
    MIN time AS time,
    host,
    AVG (IF type_instance = "free" THEN value ELSE 0) AS free,
    AVG (IF type_instance = "used" THEN value ELSE 0) AS used,
    AVG (IF type_instance = "cached" THEN value ELSE 0) AS cached,
    AVG (IF type_instance = "buffered" THEN value ELSE 0) AS buffered,
    AVG (IF type_instance LIKE "slab%" THEN value ELSE 0) AS slab
  FROM demo/collectd
  WHERE COALESCE (plugin = "memory", false)
  GROUP BY host, time // 30
  COMMIT WHEN in.time &gt; out.time + 30
  EVENT STARTING AT time WITH DURATION 30;
</pre>

<p>There are *a lot* of new things in there. Let's see them one at a time.</p>

<h3>Naming Fields</h3>

<p>Notice that we have explicitly named most of the field with the <code>AS</code> keyword.  Each field must have a name and unless Ramen can reuse an incoming field name you will have to supply the name yourself.</p>

<p>NOTE: In simple cases Ramen might come up with a name of its own making but it's not always what you want. For instance in this example the first field which value is <code>MIN time</code> would have been named "min_time", but I think "time" is more appropriate therefore I provided the name myself.</p>

<h3>Grouping and Aggregating</h3>

<p>As in SQL, the "group by" clause will define a <em>key</em> used to group the incoming tuples. This key is composed of a list of expressions. In this example we want to group tuples by hostname (in case you configured collectd on various machines) and by slices of 30 seconds. To group by time we divide the time by 30, using the integer division denoted by the double-slash operator (<code>//</code>).  The usual division (<code>/</code>) would yield a fractional number which would not map successive events into the same group.</p>

<p>In every group we compute the average of the received values (using the <code>AVG</code> aggregate function) and the minimum time (using the <code>MIN</code> aggregate function).</p>

<p>Notice that each of the measurement will be 0 if Ramen does not receive any corresponding event from collectd for that particular instance-type during the whole 30 seconds slice. This is not great but good enough for now.</p>

<p>NOTE: As in python, <code>//</code> is the _integer division_: a division where the remainder is discarded and thus the result truncated toward zero. In the above expression, the type of the result is still a float since <code>time</code> is a float, though.</p>

<h3>Windowing</h3>

<p>Every stream processor in existence come with a windowing system that basically compensate for input infiniteness. Usually, windowing boils down to a condition triggering the "closing" of the current window; in more details, what is meant by "closing" a window is: the finalization of the ongoing aggregation, the emission of a result and the emptying of the window to restart the cycle with new inputs.</p>

<p>In Ramen, the control over the windowing is very fine grained, but the above <code>COMMIT WHEN ...some condition...</code> is basically just that: when the condition is met, the current aggregation emits a result and the accumulated data is reset. Still, you should be intrigued by the condition itself: <code>in.time &gt; out.time + 30</code>. For the first time, we see that field names can be prefixed with a _tuple name_.</p>

<p>Indeed, here we are comparing the field "time" of the incoming tuples ("in.time") with the field "time" that is being computed by the aggregation (<code>MIN time AS time</code>). "in" is the name of an input tuple, while "out" is the name of the tuple computed by a group (the tuple that would be emitted shall the condition yield true). It is thus interesting to notice that those two tuples have different types: "in" has fields "time", "type_instance", "value", etc, while the output tuples have fields "time", "free", "used", etc. Both have a field named "time" so we need to prefix with the tuple name to disambiguate the expression.</p>

<p>There are many different tuples that you can address in the various clauses of an expression beside the "in" and the "out" tuple so that rich behavior can be obtained, but let's not dive into this for now. The overall meaning of this <code>COMMIT</code> expression should be clear enough: we want to aggregate the tuples until we receive a tuple which time is greater than the min time seen so far in that group, by at least 30 seconds. This assumes collectd events will be received in roughly chronological order. We could wait longer than 30s to leave some time for lagging events.</p>

<h3>Conditionals</h3>

<code>IF</code> expressions have been used to zero-out values of the wrong instance-types.  Ramen also support <code>mysql</code> type <code>IF</code> functions: <code>IF(condition, consequent, alternative)</code>, and both are just syntactic sugar for the fully fledged SQL <code>CASE</code> expression.

<p>Like in SQL but unlike in some programming languages, you can use conditionals anywhere you could use an expression; such as in the middle of a computation or as a function argument, as we did here.</p>

<h3>Event Duration</h3>

<p>Notice that we also added <code>WITH DURATION 30</code> to the description of the output event. This instruct Ramen that each tuple represents a time segment that starts at the timestamp taken from the field "time" and that represents a time slice of 30s.  This will make visualizing the timeseries more accurate.</p>

<h2>Alerting On Low Memory</h2>

<p>Ramen only ways to notify the external world of some condition is the <code>NOTIFY</code> clause that takes an HTTP URL as a parameter and that will get (as in <code>HTTP GET</code>) that URL each time the operation commits a tuple.</p>

<p>As a simple example, let's say we want to be alerted whenever the "used" memory grows beyond 50% of the total.</p>

<p>We can use the <code>NOTIFY</code> keyword to reach out to some imaginary alerting service. Let's add to <code>hosts.ramen</code> an operation named <code>memory_alert</code>, defined like this:</p>

<pre>
DEFINE memory_alert AS
  FROM memory
  SELECT
    time, host,
    free + used + cached + buffered + slab AS total,
    free * 100 / total AS used_ratio
  GROUP BY host
  COMMIT WHEN used_ratio &gt; 50
  NOTIFY "http://imaginary-alerting.com/notify?title=RAM%20is%20low%20on%20${host}&time=${time}&text=Memory%20on%20${host}%20is%20filled%20up%20to%20${used_ratio}%25"
  EVENT STARTING AT time WITH DURATION 30;
</pre>

<p>Notice that we can reuse the field <code>total</code> after it has been defined in the select clause, which comes rather handy when building complex values as it allows to name intermediary result.</p>

<p>NOTE: Should you not want such an intermediary result to be actually part of the output tuple, you would have to prepend its name with an underscore ; as a corollary, any field which name starts with an underscore will not appear in the output. Those fields are called "private fields".</p>

<p>Notice the <code>NOTIFY</code> clause: it just needs an URL within which actual field values can be inserted.</p>

<p>Let's compile that new program.</p>

</pre>
In function memory_alert: comparison (>) must not be nullable
</pre>

<p>Wait, what? Now the compiler is complaining that <code>used_ratio</code> can be NULL?  Have you noticed that all of our memory values could be NULL? That's typically the kind of surprise Ramen type system is designed to catch early.</p>

<p>Of course, collectd "type_instance" field is nullable, so is the <code>IF type_instance = "whatever"</code> conditional, so are each of the averaged memory volumes. We could wrap each use of type_instance into a <code>COALESCE</code> function but that would be tedious. Rather, let's put in practice our new knowledge about private fields. Turn the memory operation into:</p>

<pre>
DEFINE memory AS
  SELECT
    MIN time AS time,
    host,
    COALESCE (type_instance, "") AS _type,
    AVG (IF _type = "free" THEN value ELSE 0) AS free,
    AVG (IF _type = "used" THEN value ELSE 0) AS used,
    AVG (IF _type = "cached" THEN value ELSE 0) AS cached,
    AVG (IF _type = "buffered" THEN value ELSE 0) AS buffered,
    AVG (IF _type LIKE "slab%" THEN value ELSE 0) AS slab
  FROM demo/collectd
  WHERE COALESCE (plugin = "memory", false)
  GROUP BY host, time // 30
  COMMIT WHEN in.time &gt; out.time + 30
  EVENT STARTING AT time WITH DURATION 30;
</pre>

<p>...and then everything should compile and run.</p>

<p>What will happen whenever the memory usage ratio hit the threshold is that the imaginary alerting system will receive a notification from Ramen.  It would be nice if we could also tell it when the memory usage goes back below the threshold.  Let's add a boolean <code>firing</code> parameter for that purpose.</p>

<p>Edit the "memory alert" operation into this:</p>

<pre>
DEFINE memory_alert AS
  FROM hosts/memory
  SELECT
    time, host,
    free + used + cached + buffered + slab AS total,
    free * 100 / total AS used_ratio,
    used_ratio &gt; 50 AS firing
  GROUP BY host
  COMMIT AND KEEP ALL WHEN COALESCE (out.firing <> previous.firing, false)
  NOTIFY "http://imaginary-alerting.com/notify?firing=${firing}&title=RAM%20is%20low%20on%20${host}&time=${time}&text=Memory%20on%20${host}%20is%20filled%20up%20to%20${used_ratio}%25"
  EVENT STARTING AT time WITH DURATION 30;
</pre>

<p>There should be little surprise but for the commit clause.</p>

<p>There we see the "previous" tuple for the first time. It's similar to the "out" tuple, but the "out" tuple refers to the tuple that's actualy in construction (the one that would be emitted right now if that commit clauses says so) whereas the "previous" tuple refers to the former value of that tuple (the one that would have been emitted the last time we added something to that aggregation group, should the commit clause said so). The out tuple always have a value, but the previous one not necessarily: indeed, when this group have just been created there is no "last time". In that case, all the previous tuple fields would be NULL (regardless of their normal nullability). Therefore the <code>COALESCE</code>.</p>

<p>What that <code>COMMIT AND KEEP ALL</code> does is to instruct Ramen not to delete the group when the tuple is output (the default behavior is to discard the group once it's been output).  <code>KEEP ALL</code> means that the group should stay untouched, as if it hasn't been output at all. Otherwise we would loose the memory of what was the last output tuple for this host (next time we hear about that host, a new group would be created and <code>previous.firing</code> would be NULL). In contrast, <code>KEEP ALL</code> will never delete the groups, so we will have as many groups as we have hosts to save their last firing state, which is reasonable.</p>

<p>So here we send a notification only when the value of <code>firing</code> changes.  Note that in production you likely want to use an hysteresis. There are several ways to do that, but let's leave it as an exercise.</p>

<h2>Monitoring Netflows</h2>

<p>Let's now turn into netflows.</p>

<p>Have a look at the output of the <code>demo/netflow</code> operation and, armed with <a href="https://www.cisco.com/c/en/us/td/docs/net_mgmt/netflow_collection_engine/3-6/user/guide/format.html#wp1006186">netflow format reference</a>, see if you can make sense of that data.</p>

<p>If you are not already familiar with this, then you just have to know that netflows are bytes, packets and flag counts for each "flow" defined roughly as the IP socket pair (ip protocol, addresses and ports), and a "route" inside the switch from the inbound to outbound interface. Switches will send those records regularly every few minutes so that we know the volume of the traffic per socket, that we can aggregate per subnets or per switch interfaces, and so on.</p>

<p>What we are ultimately interested in, for monitoring purpose, will typically be:</p>

<ul>
<li>Is any switch interface close to saturation?</li>
<li>Is the total traffic from/to a given subnet within the expected range?</li>
<li>Is a link down?</li>
<li>Are there any traffic from a given subnet to another given subnet for a given port (for instance, from internal to external port 25)?</li>
<li>Is there some DDoS going on? Or some other malicious pattern?</li>
</ul>

<p>We will see how to compute some of those.</p>

<h3>Per interface traffic</h3>

<p>Let's start by aggregating all traffic per switch interfaces.</p>

<p>Netflow has 3 fields of interest here: "source", which is the IP address of the netflow emitter (say, a switch), and "in_iface" and "out_iface", which identifies the interfaces from which the flow entered and exited the switch.</p>

<p>To build a per interface aggregate view we therefore have to split each flow into two, saying that the traffic that have been received on interface X and emitted on interface Y count as traffic for interface X and traffic for interface Y, counting indifferently incoming and outgoing traffic.</p>

<p>Let's therefore create a new program file named "traffic.ramen", with two operations that we could name respectively "inbound" and "outbound":</p>

<pre>
DEFINE inbound AS
  SELECT source, first, last, bytes, packets, in_iface AS iface
  FROM demo/netflow;
</pre>

<p>...and...</p>

<pre>
DEFINE outbound AS
  SELECT source, first, last, bytes, packets, out_iface AS iface
  FROM demo/netflow;
</pre>

<p>Both will read the netflows and output flows with a single <code>iface</code> field for both incoming and outgoing traffic. We can then read from both those operations and have a single view of all traffic going through a given interface (in or out).</p>

<p>Let's jut do that. In an operation named "total", grouping by interface (that is, by <code>source</code> and <code>iface</code>) and aggregating the traffic (<code>bytes</code> and <code>packets</code>), until enough time has passed (300 seconds in this example):</p>

<pre>
DEFINE total AS
  FROM inbound, outbound
  SELECT
    source, iface,
    min first AS first, max last AS last,
    sum bytes AS bytes, sum packets AS packets
  GROUP BY source, iface
  COMMIT WHEN out.last - out.first &gt; 300
  EVENT STARTING AT first AND STOPPING AT last;
</pre>

<p>It might be the first time you see a FROM clause with more that one operation.  You are allowed to read from several operations as long as all these operations output (at least) all the fields that your operation needs (with the same type).</p>

<p>You could plot the "bytes" or "packets" field of this operation to get the total traffic reaching any interface.</p>

<p>For convenience let's rather compute the number of packets and bytes _per seconds_ instead:</p>

<pre>
DEFINE total AS
  FROM inbound, outbound
  SELECT
    source, iface,
    min first AS first, max last AS last,
    sum bytes / (out.last - out.first) AS bytes_per_secs,
    sum packets / (out.last - out.first) AS packets_per_secs
  GROUP BY source, iface
  COMMIT WHEN out.last - out.first &gt; 300
  EVENT STARTING AT first AND STOPPING AT last;
</pre>

<p>Notice the prefix in <code>out.first</code> and <code>out.last</code> to identify the computed <code>first</code> and <code>last</code> from the output tuple ; without the prefix Ramen would have used the <code>first</code> and <code>last</code> fields from the input tuple instead of the result of the <code>min</code>/<code>max</code> aggregation functions, as the input tuple (<code>in</code>) is the default when the same field can have several origins.</p>

<p>Now that we have the bandwidth per interface every 5 minutes, it is easy to signal when the traffic is outside the expected bounds for too long.  But we can do a bit better. Let's append this to <code>traffic.ramen</code>:</p>

<pre>
DEFINE traffic_alert AS
  FROM total
  SELECT
    source, iface,
    (last - first) / 2 AS time,
    bytes_per_secs,
    5-ma locally (bytes_per_secs &lt; 100 OR bytes_per_secs &gt; 8e3) &gt;= 4 AS firing
  GROUP BY source, iface
  COMMIT AND KEEP ALL WHEN COALESCE (out.firing <> previous.firing, false)
  NOTIFY "http://imaginary-alerting.com/notify?firing=${firing}&title=Traffic%20on%20${source}%2F${iface}&time=${time}";
</pre>

<p>Notice the definition of firing: instead of merely fire whenever the average traffic over 5 minutes is outside the range, we do this enigmatic "5-ma" dance. "5-ma" is actually a function that performs a moving average, ie. the average of the last 5 values. In order to average boolean values those will be converted into floats (1 for true and 0 for false as usual). So if the average of the last 5 values is above or equal to 4 that means at least 4 of the latests 5 values were true. Therefore, at the expense of a bit more latency, we can skip over a flapping metric.</p>

<p>NOTE: of course there are also functions for <code>2-ma</code>, <code>3-ma</code> and so on. This is just syntactic sugar.</p>

<p>The next enigmatic bit is the "locally" keyword. This is a function modifier that means that instead of storing it's temporary state globally the "5-ma" function should have one such state per group ; in other words, instead of computing the average over the last 5 incoming tuples regardless of their key, it should compute the average over the last 5 tuples aggregated into the same group.  Some functions default to having a global context while some default to have a local context. If unsure, add either <code>locally</code> or <code>globally</code> after any stateful function.</p>

<h3>Alerting on link down</h3>

<p>Alerting on link down might seems easy - isn't it a special case of the above, when we test for <code>bytes_per_secs = 0</code> ?  This won't work for a very interesting reason: When there is no traffic at all on an interface, switches will not send a netflow message with zero valued counters. Instead they will not send anything at all, thus stalling the stream processor. To detect link down, therefore, we need some timeout.</p>

<p>Assuming we will still receive some netflows even when one link is down, the easier solution seems to compare the last time of each group with the latest input tuple time, each time a tuple is received, like so:</p>

<pre>
DEFINE link_down_alert AS
  FROM traffic/total
  SELECT source, iface, max last
  GROUP BY source, iface
  COMMIT AND KEEP ALL WHEN in.last - max_last &gt; 300
  NOTIFY "http://imaginary-alerting.com/notify?text=link%20${source}%2F${iface}%20is%20down";
</pre>

<p>...and you have it!</p>

<p>If you have many interfaces, comparing each group with each incoming netflow might not be very efficient though. Maybe Ramen should provide a proper timeout feature, either based on actual wall clock time or on event time?</p>

<p>You should now be able to survive given only the <a href="language_reference.html">language reference manual</a>.</p>

</body>
</html>
